{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS2_Final_Project_git.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1sgCq38xQrdD0ew_u9OP7cDgbeD37Kz9a",
      "authorship_tag": "ABX9TyNIetyvaSKregiuD9wk0Zpe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trace55/Tweet_generation_classification/blob/main/DS2_Final_Project_git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl0bK1szhthf"
      },
      "source": [
        "! pip install -U transformers datasets sentencepiece pandas scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlXwOZTzLVEt"
      },
      "source": [
        "import re\n",
        "\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "def Convert(string):\n",
        "    li = list(string.split(\"_____?_____.\"))\n",
        "    return li\n",
        "\n",
        "def clean_tw_df(df): \n",
        "    data = df['tweet'].str.cat(sep= '_____?_____.')\n",
        "    data = re.sub('(?:\\s)@[^, ]*', '', data)\n",
        "    file_name = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', data, flags=re.MULTILINE)\n",
        "    file_name = re.sub(r'http\\S+', '', file_name)\n",
        "    df_txt = deEmojify(file_name)\n",
        "    df_ls = Convert(df_txt)\n",
        "    df = pd.DataFrame(df_ls, columns = ['tweet'])\n",
        "    return df\n",
        "\n",
        "def clean_tw_to_file(df, name):\n",
        "    data = df['tweet'].str.cat(sep= '\\n')\n",
        "    data = re.sub('(?:\\s)@[^, ]*', '', data)\n",
        "    file_name = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', data, flags=re.MULTILINE)\n",
        "    file_name = re.sub(r'http\\S+', '', file_name)\n",
        "    df_txt = deEmojify(file_name)\n",
        "    x = file_name\n",
        "    with open(f'{name}.txt', 'w+') as fh:\n",
        "        fh.write(x)\n",
        "    X = f'{name}.txt'\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe8Wr_3ulFQ8",
        "outputId": "0559b753-d522-4b00-824b-3dcd1d316907"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path1= '/content/drive/MyDrive/D.S 2 Project/pro_mask.csv'\n",
        "p_df = pd.read_csv(path1)\n",
        "\n",
        "path2= '/content/drive/MyDrive/D.S 2 Project/anti_mask.csv'\n",
        "a_df = pd.read_csv(path2)\n",
        "\n",
        "anti_df = clean_tw_df(a_df)\n",
        "pro_df = clean_tw_df(p_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,4,11,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSSwtmb6h2vF"
      },
      "source": [
        "import pathlib, shutil, dataclasses, contextlib, torch, numpy as np, pandas as pd\n",
        "from typing import Any\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import Trainer, TrainingArguments, pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        " \n",
        "@dataclasses.dataclass\n",
        "class NLP:\n",
        "    model_checkpoint: str = 'distilbert-base-uncased'\n",
        "    model_base:       Any = AutoModelForSequenceClassification\n",
        "    task:             str = 'text-generation'\n",
        "    path:             str = '/content/drive/MyDrive/D.S 2 Project/'\n",
        "    learning_rate:    float = 1e-5\n",
        "    batch_size:       int = 32\n",
        "    warmup:           int = 600\n",
        "    max_seq_length:   int = 128\n",
        "    num_epochs:       int = 35\n",
        "    # values copied from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.path = pathlib.Path(self.path)\n",
        "        self.path_model = self.path / f'{self.task}-{self.model_checkpoint.split(\"/\")[-1]}'\n",
        "        self.path_model.mkdir(parents=True, exist_ok=True)\n",
        "        self.model_config = {'pretrained_model_name_or_path':self.model_checkpoint,\n",
        "                             'max_length':self.max_seq_length,\n",
        "                             'task_specific_params': {'text-generation': {'do_sample': True, 'max_length': self.max_seq_length}},}\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint, use_fast=True, model_max_length=self.max_seq_length)\n",
        "        if self.tokenizer._pad_token is None:\n",
        "            self.tokenizer._pad_token = self.tokenizer._eos_token\n",
        " \n",
        "    def __getitem__(self, key):\n",
        "        return getattr(self, key)\n",
        " \n",
        "    def __setitem__(self, key, value):\n",
        "        return setattr(self, key, value)\n",
        " \n",
        "    def _listify(self, x):\n",
        "        if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, range):\n",
        "            return list(x)\n",
        "        else:\n",
        "            return [x]\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        tokens = self.tokenizer(text, truncation=False, padding='max_length', return_tensors='pt')\n",
        "        T = tokens['input_ids'].shape[1]\n",
        "        sh = (T // self.max_seq_length, self.max_seq_length)\n",
        "        T = np.product(sh)\n",
        "        return {key:val[0,:T].reshape(sh) for key, val in tokens.items()}\n",
        "\n",
        "    def splitter(self, ds, frac=0.2):\n",
        "        frac = min(frac, 1-frac)\n",
        "        idxes = next(StratifiedShuffleSplit(test_size=frac).split(ds['labels'], ds['labels']))\n",
        "        return [ds.select(idx) for idx in idxes]\n",
        "\n",
        "    def make_dataset(self, source_data, text_col='text', label_col='label', holdout_frac=0.1, validate_frac=0.2):\n",
        "        if label_col not in source_data.columns:\n",
        "            print(f'Label column {label_col} not found - filling with 0')\n",
        "            source_data[label_col] = 0\n",
        "        self.df = pd.DataFrame(source_data).dropna(subset=[text_col, label_col]).sample(frac=1.0).reset_index(drop=True)\n",
        "\n",
        "        tokens = dict()\n",
        "        for label, text in self.df[[label_col, text_col]].values:\n",
        "            tensors = self.preprocess(text)\n",
        "            tensors['labels'] = [label for _ in range(tensors['input_ids'].shape[0])]\n",
        "            for key, val in tensors.items():\n",
        "                try:\n",
        "                    tokens[key].append(val)\n",
        "                except:\n",
        "                    tokens[key] = [val]\n",
        "        L = np.hstack(tokens.pop('labels'))\n",
        "        id2label = dict(enumerate(np.unique(L)))\n",
        "        label2id = {l:i for i,l in id2label.items()}\n",
        "        tokens = {key: torch.row_stack(ls) for key, ls in tokens.items()}\n",
        "        tokens['labels'] = [label2id[l] for l in L]\n",
        "        self.model_config['id2label'] = id2label\n",
        "        self.model_config['label2id'] = label2id\n",
        "\n",
        "        D = DatasetDict({'train': Dataset.from_dict(tokens)})\n",
        "        for nm, frac in (('holdout',holdout_frac), ('validate',validate_frac)):\n",
        "            if frac > 0 and frac <=1:\n",
        "                D['train'], D[nm] = self.splitter(D['train'], frac)\n",
        "                \n",
        "        self.dataset = D\n",
        "        if self.task == 'text-generation':\n",
        "            def f(X):\n",
        "                X['labels'] = X['input_ids']\n",
        "                return X\n",
        "            self.dataset = D.map(f)\n",
        "        return self.dataset\n",
        "\n",
        "    def make_model(self, overwrite=False):\n",
        "        if overwrite:\n",
        "            shutil.rmtree(self.path_model, ignore_errors=True)\n",
        "\n",
        "        with contextlib.suppress(OSError):\n",
        "            self.model = self.model_base.from_pretrained(self.path_model)\n",
        "            print(f'Using existing model at {str(self.path_model)}')\n",
        "\n",
        "        if not hasattr(self, 'model'):\n",
        "            print(f'No model found at {str(self.path_model)} - creating new one')\n",
        "            assert hasattr(self, 'dataset'), 'No dataset found - please run make_dataset first'\n",
        "            self.train(self.dataset, overwrite)\n",
        "            self.model.save_pretrained(self.path_model)\n",
        "\n",
        "        self.model = self.model.to('cpu')\n",
        "        self.pipe = pipeline(task=self.task, model=self.model, tokenizer=self.tokenizer)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = self.tokenizer(text, truncation=False, return_tensors='pt')\n",
        "        s = self.max_seq_length\n",
        "        prefix = {key:val[:,:-s] for key, val in tokens.items()}\n",
        "        suffix = {key:val[:,-s:] for key, val in tokens.items()}\n",
        "        return tokens, prefix, suffix\n",
        "\n",
        "    def generate(self, prompts=\"\", max_new_tokens=20, **kwargs):\n",
        "        prompts = self._listify(prompts)\n",
        "        outputs = []\n",
        "        for text in prompts:\n",
        "            tokens, prefix, suffix = self.tokenize(text)\n",
        "            t = tokens['input_ids'].shape[1]\n",
        "            if t > self.max_seq_length:\n",
        "                print(f'num_tokens {t} > model max_seq_length {self.max_seq_length}; using the tail for prompt')\n",
        "            L = max_new_tokens+suffix['input_ids'].shape[1]\n",
        "            gen = self.model.generate(**suffix, max_length=L, **kwargs)\n",
        "            o = torch.cat((prefix['input_ids'],gen), axis=1)\n",
        "            d = self.tokenizer.decode(o[0])\n",
        "            outputs.append(d)\n",
        "        return outputs if len(outputs) > 1 else outputs[0]\n",
        "\n",
        "    def train(self, dataset=None, overwrite=False):\n",
        "        if dataset is None:\n",
        "            dataset = self.dataset\n",
        "        self.path_model.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir                  = self.path_model,\n",
        "            overwrite_output_dir        = overwrite,\n",
        "            evaluation_strategy         = 'epoch',\n",
        "            logging_strategy            = 'no',\n",
        "            logging_dir                 = self.path_model / 'logs',\n",
        "            save_strategy               = 'no',\n",
        "            load_best_model_at_end      = False,\n",
        "            learning_rate               = self.learning_rate,\n",
        "            per_device_train_batch_size = self.batch_size,\n",
        "            per_device_eval_batch_size  = self.batch_size,\n",
        "            warmup_steps                = self.warmup,\n",
        "            num_train_epochs            = self.num_epochs,\n",
        "        )\n",
        "\n",
        "        self.model = self.model_base.from_pretrained(**self.model_config)\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args =training_args,\n",
        "            train_dataset=dataset['train'],\n",
        "            eval_dataset =dataset['validate'],)\n",
        "        trainer.train()\n",
        "\n",
        "base_path = '/content/drive/MyDrive/D.S 2 Project/'\n",
        "data_kwargs  = {'text_col'        : 'tweet',\n",
        "                'label_col'       : 'view',\n",
        "                'holdout_frac'    : 0.0,\n",
        "                'validate_frac'   : 0.2,}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpho4rMb3e1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pro_df = pro_df.sample(anti_df.shape[0],random_state =41) #make same sizes\n",
        "pro_df['view'] = 'pro'\n",
        "anti_df['view'] = 'anti'\n",
        "all_data = pd.concat([pro_df,anti_df])\n",
        "all_data = all_data.sample(frac=1).reset_index(drop=True)\n",
        "df_train, df_hold = train_test_split(all_data, test_size=0.01, random_state= 41)\n",
        "source_data = df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQvB6QMWh9E-"
      },
      "source": [
        "# Text generation\n",
        "model_kwargs = {'model_checkpoint': 'distilgpt2',\n",
        "                'model_base'      : AutoModelForCausalLM,\n",
        "                'task'            : 'text-generation',\n",
        "                # can adjust training parameters here such as num_epochs & batch size - see first lines of class NLP definition\n",
        "                }\n",
        "gens = dict()\n",
        "for g, df in source_data.groupby('view'):\n",
        "    gen = NLP(**model_kwargs, path=f'{base_path}/{g}')\n",
        "    gen.make_dataset(df, **data_kwargs)\n",
        "    gen.make_model()\n",
        "    gens[g] = gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCM6IwgnHbB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60221060-f64c-4631-ba11-15df67282994"
      },
      "source": [
        "## Text-Classifier\n",
        "\n",
        "model_kwargs = {'model_checkpoint': 'distilbert-base-uncased',\n",
        "                'model_base'      : AutoModelForSequenceClassification,\n",
        "                'task'            : 'sentiment-analysis',\n",
        "                # can adjust training parameters here such as num_epochs & batch size - see first lines of class NLP definition\n",
        "                }\n",
        "clf = NLP(**model_kwargs, path=base_path)\n",
        "clf.make_dataset(source_data, **data_kwargs)\n",
        "clf.make_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using existing model at /content/drive/MyDrive/D.S 2 Project/sentiment-analysis-distilbert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJSJ4AWAHfNE"
      },
      "source": [
        "gen_kwargs = {'repetition_penalty':10.0,  # penalty for repeating yourself\n",
        "              'max_new_tokens':10,  # max tokens to add for each call to generate\n",
        "\n",
        "              }\n",
        "\n",
        "min_words = 50  # stop when this many words are in text\n",
        "\n",
        "dff = pd.DataFrame()\n",
        "\n",
        "for j in range(df_hold.shape[0]):\n",
        "    for g,gen in gens.items(): \n",
        "        text = \" \".join(df_hold['tweet'].iloc[j].split()[:8])\n",
        "        for i in range(10):  # run generate up to this many times to try to get min_words\n",
        "            text = gen.generate(text, **gen_kwargs)  # call generate\n",
        "            L = len(text.split(\" \"))  # compute number of words by splitting on spaces\n",
        "            print(f'{i,L}\\n{text}\\n\\n')\n",
        "            if L >= min_words:  # stop if enough words\n",
        "                ls = [text]\n",
        "                ls.append(g)\n",
        "                ls = [ls]\n",
        "                print(ls)\n",
        "                dff1 = pd.DataFrame(ls, columns = ['tweet','view'])\n",
        "                dff = pd.concat([dff1,dff])\n",
        "                break\n",
        "        clf.pipe(text)  # classify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bd5Zhtr_cQv"
      },
      "source": [
        "gen_kwargs = {'repetition_penalty':10.0,  # penalty for repeating yourself\n",
        "              'max_new_tokens':10,  # max tokens to add for each call to generate\n",
        "\n",
        "              }\n",
        "\n",
        "min_words = 50 \n",
        "dff = pd.DataFrame()\n",
        "for k in range(700):\n",
        "    for g,gen in gens.items(): \n",
        "        text = \" \"\n",
        "        for i in range(10):  # run generate up to this many times to try to get min_words\n",
        "            text = gen.generate(text, **gen_kwargs)  # call generate\n",
        "            L = len(text.split(\" \"))  # compute number of words by splitting on spaces\n",
        "            print(f'{i,L}\\n{text}\\n\\n')\n",
        "            if L >= min_words:  # stop if enough words\n",
        "                ls = [text]\n",
        "                ls.append(g)\n",
        "                ls = [ls]\n",
        "                print(ls)\n",
        "                dff1 = pd.DataFrame(ls, columns = ['tweet','view'])\n",
        "                dff = pd.concat([dff1,dff])\n",
        "                break\n",
        "        clf.pipe(text)  # classify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MQBKEPKAGP1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "285MCDNJS9BY"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGXBpZqGo_Ne"
      },
      "source": [
        "dff = dff.reset_index(drop=True)\n",
        "dff.tail(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PzP0pr4ZewB"
      },
      "source": [
        "c = 0\n",
        "for k in range(dff.shape[0]):\n",
        "    lab = clf.pipe(dff['tweet'].iloc[k])[0]['label']\n",
        "    if lab == dff['view'].iloc[k]:\n",
        "        c = c+1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwLV5JLGlCll",
        "outputId": "01ca6e05-bef6-4328-81d7-e11de888a659"
      },
      "source": [
        "c/dff.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.895924308588064"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOc_mRTOggpG"
      },
      "source": [
        "dff.tail(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up7Fqhfay5Ud"
      },
      "source": [
        "df_hold.tail(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4MN701SmUVi"
      },
      "source": [
        "df_hold.head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ki__Rm6Zs7P"
      },
      "source": [
        "c = 0\n",
        "for k in range(df_hold.shape[0]):\n",
        "    lab = clf.pipe(df_hold['tweet'].iloc[k])[0]['label']\n",
        "    if lab == df_hold['view'].iloc[k]:\n",
        "        c = c+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saKbvPpNd8PS",
        "outputId": "12ffd703-2bef-4a12-beb2-1184cd478bec"
      },
      "source": [
        "c/df_hold.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.981675392670157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Vwg-Z5Ihpm"
      },
      "source": [
        "##Results:\n",
        "~ 88% On Classifier\n",
        "~ 90% On Generation"
      ]
    }
  ]
}